"GMMBoost","bst","sboost","C50",
"kernlab","svmpath","nnet","gnn","rnn",
"spnn","brnn","RSNNS","AMORE","simpleNeural",
"ANN2","yap","yager","deep","neuralnet",
"TeachNet","deepnet","RcppDL","tensorflow","h2o",
"kerasR","deepNN","Buddle","automl","RLT",
"ReinforcementLearning","MDPtoolbox","lime","localModel",
"iml","flashlight","interpret","outliertree",
"dockerfiler", "sparklyr","cloudml","ggvis",
"htmlwidgets","maps","sunburstR","lattice","predict3d",
"rgl","rglwidget","plot3Drgl","ggmap","ggplot2",
"plotly","RColorBrewer","dygraphs","canvasXpress","qgraph",
"moveVis","ggcharts","visNetwork","visreg","sjPlot",
"squash","mlr3viz","DiagrammeR",
"pavo","rasterVis","timelineR","DataViz","d3r","breakDown",
"dashboard","highcharter","rbokeh","rvest",
"Rcrawler","ralger","scrapeR","devtools","usethis",
"roxygen2","knitr","rmarkdown","flexdashboard","shiny",
"xtable","httr","profvis")
# create list of missing packages
new.packages <- AllPackages[!(AllPackages %in% installed.packages()[,"Package"])]
# install missing packages
if(length(new.packages)) install.packages(new.packages)
# load all recommended packages
lapply(AllPackages, library, character.only = TRUE)
n
# Recommended R Libraries
# 0. All In One Cammand Wrap-Up
# from https://www.r-bloggers.com/2020/04/essential-list-of-useful-r-packages-for-data-scientists/
# List of recommended packages
AllPackages <- c("Hmisc","foreign","protr","readxl", "readODS",
"csv","readr","tidyverse","jsonlite","rjson",
"RJSONIO","jsonvalidate","sparkavro","arrow","feather",
"XML","odbc","RODBC","mssqlR","RMySQL",
"postGIStools","RPostgreSQL","odbc", "remotes",
"RSQLite","sqliter","dbflobr","RSQL","sqldf",
"queryparser","influxdbr","janitor","outliers",
"missForest","frequency","Amelia","diffobj","mice", "VIM",
"mi","wrangle","mitools",
"stringr","lubridate","glue","scales","hablar",
"dplyr","magrittr","data.table","plyr",
"tidyr","tibble","reshape2","stats","lars",
"caret","survival","gam","glmnet","quantreg",
"sgd","BLR","MASS","car","mlogit","RRedshiftSQL",
"earth","faraway","nortest","lmtest","nlme",
"splines","sem","pls","tree","rpart","rio",
"FuzzyNumbers","ez","psych","pastecs","CCA","CCP",
"icapca","gvlma","smacof","MVN","rpca",
"EFA.MRFA","MFAg","MVar","fabMix", "fad","spBFA","mnlfa",
"GFA","lmds","SPCALDA","semds","superMDS",
"vcd","vcdExtra","ks","rrcov","eRm",
"MNP","bayesm","ltm","fpc","cluster",
"treeClust","e1071","NbClust","skmeans","kml",
"compHclust","protoclust","pvclust","genie","tclust",
"ClusterR","dbscan","CEC","GMCM","EMCluster",
"randomLCA","MOCCA","factoextra","poLCA",
"zoo","xts","timeSeries","tsModel","TSMining",
"TSA","fma","fpp2","fpp3","tsfa",
"TSdist","TSclust","feasts","MTS","dse",
"sazedR","kza","fable","forecast","tseries",
"nnfor","quantmod","fastnet","tsna","sna",
"networkR","InteractiveIGraph","SemNeT","igraph",
"dyads","staTools","CINNA","tm","tau","NetworkToolbox",
"koRpus","lexicon","sylly","textir","textmineR",
"MediaNews","lsa","ngram","ngramrr","corpustools",
"udpipe","textstem","tidytext","text2vec","crossval",
"klaR","gencve","cvAUC",
"CVThresh","cvTools","dcv","cvms","blockCV",
"randomForest","grf","ipred","party","randomForestSRC",
"BART","Boruta","LTRCtrees","REEMtree",
"binomialRF","superml","gbm",
"GMMBoost","bst","sboost","C50",
"kernlab","svmpath","nnet","gnn","rnn",
"spnn","brnn","RSNNS","AMORE","simpleNeural",
"ANN2","yap","yager","deep","neuralnet",
"TeachNet","deepnet","RcppDL","tensorflow","h2o",
"kerasR","deepNN","Buddle","automl","RLT",
"ReinforcementLearning","MDPtoolbox","lime","localModel",
"iml","flashlight","interpret","outliertree",
"dockerfiler", "sparklyr","cloudml","ggvis",
"htmlwidgets","maps","sunburstR","lattice","predict3d",
"rgl","rglwidget","plot3Drgl","ggmap","ggplot2",
"plotly","RColorBrewer","dygraphs","canvasXpress","qgraph",
"moveVis","ggcharts","visNetwork","visreg","sjPlot",
"squash","mlr3viz","DiagrammeR",
"pavo","rasterVis","timelineR","DataViz","d3r","breakDown",
"dashboard","highcharter","rbokeh","rvest",
"Rcrawler","ralger","scrapeR","devtools","usethis",
"roxygen2","knitr","rmarkdown","flexdashboard","shiny",
"xtable","httr","profvis")
# create list of missing packages
new.packages <- AllPackages[!(AllPackages %in% installed.packages()[,"Package"])]
# install missing packages
if(length(new.packages)) install.packages(new.packages)
# load all recommended packages
lapply(AllPackages, library, character.only = TRUE)
# Quick Install DataBionics CRAN-Packages
# DataBionics Faculty - AG Datenbionik
# FB 12 - Mathematics and Computer Science
# Philipps University of Marburg, Germany
# List of CRAN(!) packages
AllDBPackages <- c('GeneralizedUmatrix', 'DatabionicSwarm', 'DataVisualizations',
'AdaptGauss', 'ABCanalysis', 'ProjectionBasedClustering','FCPS')
# create list of missing packages
new.DB.packages <- AllDBPackages[!(AllDBPackages %in% installed.packages()[,"Package"])]
# install missing packages
if(length(new.DB.packages)) {
install.packages(new.DB.packages)
}
# load all recommended packages
lapply(AllDBPackages, library, character.only = TRUE)
# Quick Install DataBionics CRAN-Packages
# DataBionics Faculty - AG Datenbionik
# FB 12 - Mathematics and Computer Science
# Philipps University of Marburg, Germany
# List of CRAN(!) packages
AllDBPackages <- c('GeneralizedUmatrix', 'DatabionicSwarm', 'DataVisualizations',
'AdaptGauss', 'ABCanalysis', 'ProjectionBasedClustering','FCPS')
# create list of missing packages
new.DB.packages <- AllDBPackages[!(AllDBPackages %in% installed.packages()[,"Package"])]
# install missing packages
if(length(new.DB.packages)) {
install.packages(new.DB.packages)
}
# load all recommended packages
lapply(AllDBPackages, library, character.only = TRUE)
# Credible Visualization for Two-Dimensional Projections of Data
# https://github.com/Mthrun/GeneralizedUmatrix
# remotes::install_github("Mthrun/GeneralizedUmatrix")
install.packages("GeneralizedUmatrix",dependencies = TRUE)
install.packages("GeneralizedUmatrix", dependencies = TRUE)
# The Databionic swarm is an unsupervised machine learning method for cluster analysis and the visualization of structures of high-dimensional data.
# https://github.com/Mthrun/DatabionicSwarm
# remotes::install_github("Mthrun/DatabionicSwarm")
install.packages("DatabionicSwarm",dependencies = TRUE)
install.packages("DatabionicSwarm", dependencies = TRUE)
# Time Series Analysis Tools (TSAT) can be used to describe event-pattern detection as a part of Complex event processing (CEP) for categorial time series and gives several approaches for numerical times series, like Filtering through FFT, WVT or predictions (e.g. compound model).
# https://github.com/Mthrun/TSAT
remotes::install_github('mthrun/TSAT')
# Hidden Markov Models
# https://github.com/Mthrun/RHmm
remotes::install_github('mthrun/RHmm')
# Databionics Toolbox for the Analysis of Fluorescence-Activated Cell Sorting (FACS) Data
# https://github.com/Mthrun/dbt.FlowCytometry
remotes::install_github('mthrun/dbt.FlowCytometry')
# Databionics Toolbox for the Analysis of Fluorescence-Activated Cell Sorting (FACS) Data
# https://github.com/Mthrun/dbt.FlowCytometry
remotes::install_github('mthrun/dbt.FlowCytometry')
# Common Supervised Maschine Learning algorithms
# https://github.com/Mthrun/Classifiers
remotes::install_github('mthrun/Classifiers')
# https://github.com/aultsch/DataIO
remotes::install_github('aultsch/DataIO')
# Functions For Different Distance Measures
# https://github.com/Mthrun/Distances
remotes::install_github('mthrun/Distances')
# Updating R and R packages from CRAN
#verify R to be up to date
if (!require('installr')) install.packages('installr')
library(installr)
updateR()
# list all packages where an update is available
old.packages()
# update all available packages
update.packages(ask = FALSE)
install.packages("spatialEco")
library(spatialEco)
random_data_sample <- runif(1000)
confinterval4mean <- conf.interval(random_data_sample, cl = 0.95,  stat = "mean", std.error = TRUE)
print(confinterval4mean)
confinterval4mean
# Beispiel 1: sample size = 1000
random_data_sample1 <- runif(1000)
confinterval4mean <- conf.interval(random_data_sample1, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean
confinterval4mean1 <- conf.interval(random_data_sample1, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean1
e = 100
random_data_sample_smallN <- runif(1000)
confinterval4mean_smallN <- conf.interval(random_data_sample_smallN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_smallN
random_data_sample_bigN <- runif(1000)
confinterval4mean_bigN <- conf.interval(random_data_sample_bigN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_bigN
# install.packages("spatialEco")
# see: https://cran.r-project.org/package=spatialEco
library(spatialEco)
# Beispiel 1: sample size = 1000
random_data_sample_bigN <- runif(1000)
confinterval4mean_bigN <- conf.interval(random_data_sample_bigN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_bigN
# Beispiel 2: sample size = 100
random_data_sample_smallN <- runif(1000)
confinterval4mean_smallN <- conf.interval(random_data_sample_smallN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_smallN
# Explanation of results:
# lci
d <- density(random_data_sample_bigN)
plot(d, type="n", main = "PDF with mean and 0.95 confidence interval")
polygon(d, col="cyan3")
abline(v=mean(x, na.rm = TRUE), lty = 2)
segments( x0=cr[["lci"]], y0=mean(d$y), x1=cr[["uci"]],
y1=mean(d$y), lwd = 2.5,
col = "black")
segments( x0=confinterval4mean_bigN[["lci"]], y0=mean(d$y), x1=confinterval4mean_bigN[["uci"]],
y1=mean(d$y), lwd = 2.5,
col = "black")
legend("topright", legend = c("mean", "CI"),
lty = c(2,1), lwd = c(1,2.5))
d <- density(random_data_sample_bigN)
plot(d, type="n", main = "PDF with mean and 0.95 confidence interval")
polygon(d, col="cyan3")
abline(v=mean(x, na.rm = TRUE), lty = 2)
segments( x0=confinterval4mean_bigN[["lci"]], y0=mean(d$y), x1=confinterval4mean_bigN[["uci"]],
y1=mean(d$y), lwd = 2.5,
col = "black")
legend("topright", legend = c("mean", "ConfInterval"),
lty = c(2,1), lwd = c(1,2.5))
d <- density(random_data_sample_bigN)
plot(d, type="n", main = "PDF with mean and 0.95 confidence interval")
polygon(d, col="cyan3")
abline(v=mean(x, na.rm = TRUE), lty = 2)
segments( x0=confinterval4mean_bigN[["lci"]], y0=mean(d$y), x1=confinterval4mean_bigN[["uci"]],
y1=mean(d$y), lwd = 2.5,
col = "black")
legend("topright", legend = c("mean", "ConfInterval"),
lty = c(2,1), lwd = c(1,2.5))
x <- random_data_sample_bigN
se <- function(x) { sqrt(stats::var(x, na.rm = TRUE) / length(stats::na.omit(x)))}
se
se(x)
e = stats::qt(cl, df = length(x)-1) * se(x)
e = stats::qt(0.95, df = length(x)-1) * se(x)
cie <- mean(x) + c(-e, e)
cie
mean(x) + -e
# Manuell korrekt nachberechnet:
linkerFehler <- mean(x) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
linkerFehler
rechterFehler <- mean(x) + (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
rechterFehler
mean(random_data_sample_bigN)
# rechterFehler
mean(x) + (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# Aufgabe 1
# a)
# install.packages("spatialEco")
# see: https://cran.r-project.org/package=spatialEco
library(spatialEco)
# function 'conf.interval' is used to calculate confidence interval for mean of a distribution with unknown variance
# Beispiel I: sample size = 1000
random_data_sample_bigN <- runif(1000)
confinterval4mean_bigN <- conf.interval(random_data_sample_bigN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_bigN
# Beispiel II: sample size = 100
random_data_sample_smallN <- runif(1000)
confinterval4mean_smallN <- conf.interval(random_data_sample_smallN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_smallN
# Explanation of results:
# lci = lower confidence interval value
## Berechnung in R mittels quantile function ('qt') aus dem enthaltenen 'stats' package
## (siehe https://stat.ethz.ch/R-manual/R-devel/library/stats/html/TDist.html):
## Pseudocode: estimate = stats::qt(conf_level = 0.95, data = length(sample N - 1) * std.error(sample N), sodass
## Formel: lower confidence interval value = mean(sample N)  - estimate
## in R: lci = mean(sample N) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# uci = upper confidence interval value
## analog zu lci aber als Addition
## upper confidence interval value = mean(sample N) + estimate
# mean = mean value of distribution
## Berechnung: mean(sample N)
# std.error = standard error of distribution
## Beachte: für die Berechnung der Standardabweichung wird die genormte Standardabweichung benutzt:
## Formel: std.error = sqrt(variance/length(sample N))
## in R: std.error = sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# lci: I --> 0.4721724; II --> 0.4868976
# uci: I --> 0.5016414; II --> 0.5171098
# mean: I --> 0.4869069; II --> 0.5020037
# std.error: I --> 0.008949626; II --> [1] 0.009175342
# Je größer N desto kleiner sind die Werte von linkem/rechtem Fehler, Standardabweichung und Mittelwert!
# Manuell korrekt nachberechnet:
# linkerFehler
mean(x) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# rechterFehler
mean(x) + (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# Mittelwert
mean(random_data_sample_bigN)
# Manuell korrekt nachberechnet:
# linkerFehler
mean(random_data_sample_bigN) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# rechterFehler
mean(random_data_sample_bigN) + (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# Aufgabe 1
# a)
# install.packages("spatialEco")
# see: https://cran.r-project.org/package=spatialEco
library(spatialEco)
# function 'conf.interval' is used to calculate confidence interval for mean of a distribution with unknown variance
# Beispiel I: sample size = 1000
random_data_sample_bigN <- runif(1000)
confinterval4mean_bigN <- conf.interval(random_data_sample_bigN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_bigN
# Beispiel II: sample size = 100
random_data_sample_smallN <- runif(1000)
confinterval4mean_smallN <- conf.interval(random_data_sample_smallN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_smallN
# Explanation of results:
# lci = lower confidence interval value
## Berechnung in R mittels quantile function ('qt') aus dem enthaltenen 'stats' package
## (siehe https://stat.ethz.ch/R-manual/R-devel/library/stats/html/TDist.html):
## Pseudocode: estimate = stats::qt(conf_level = 0.95, data = length(sample N - 1) * std.error(sample N), sodass
## Formel: lower confidence interval value = mean(sample N)  - estimate
## in R: lci = mean(sample N) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# uci = upper confidence interval value
## analog zu lci aber als Addition
## upper confidence interval value = mean(sample N) + estimate
# mean = mean value of distribution
## Berechnung: mean(sample N)
# std.error = standard error of distribution
## Beachte: für die Berechnung der Standardabweichung wird die genormte Standardabweichung benutzt:
## Formel: std.error = sqrt(variance/length(sample N))
## in R: std.error = sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# lci: I --> 0.4721724; II --> 0.4868976
# uci: I --> 0.5016414; II --> 0.5171098
# mean: I --> 0.4869069; II --> 0.5020037
# std.error: I --> 0.008949626; II --> [1] 0.009175342
# Je größer N desto kleiner sind die Werte von linkem/rechtem Fehler, Standardabweichung und Mittelwert!
# Manuell korrekt nachberechnet:
# linkerFehler
mean(random_data_sample_bigN) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# rechterFehler
mean(random_data_sample_bigN) + (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# Mittelwert
mean(random_data_sample_bigN)
# Angepasst an unsymmetrische Fehlerverteilung, d.h. links 1% und rechts 4%:
# links
mean(random_data_sample_bigN) - (stats::qt(0.99, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# rechts
mean(random_data_sample_bigN) + (stats::qt(0.96, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# Aufgabe 1
# a)
# install.packages("spatialEco")
# see: https://cran.r-project.org/package=spatialEco
library(spatialEco)
# function 'conf.interval' is used to calculate confidence interval for mean of a distribution with unknown variance (see: https://github.com/jeffreyevans/spatialEco/blob/master/R/conf.interval.R)
# Beispiel I: sample size = 1000
random_data_sample_bigN <- runif(1000)
confinterval4mean_bigN <- conf.interval(random_data_sample_bigN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_bigN
# Beispiel II: sample size = 100
random_data_sample_smallN <- runif(1000)
confinterval4mean_smallN <- conf.interval(random_data_sample_smallN, cl = 0.95,  stat = "mean", std.error = TRUE)
confinterval4mean_smallN
# Explanation of results:
# lci = lower confidence interval value
## Berechnung in R mittels quantile function ('qt') aus dem enthaltenen 'stats' package
## (siehe https://stat.ethz.ch/R-manual/R-devel/library/stats/html/TDist.html):
## Pseudocode: estimate = stats::qt(conf_level = 0.95, data = length(sample N - 1) * std.error(sample N), sodass
## Formel: lower confidence interval value = mean(sample N)  - estimate
## in R: lci = mean(sample N) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# uci = upper confidence interval value
## analog zu lci aber als Addition
## upper confidence interval value = mean(sample N) + estimate
# mean = mean value of distribution
## Berechnung: mean(sample N)
# std.error = standard error of distribution
## Beachte: Wir benutzen die Standardverteilung der theoretischen Distribution des sample Mittelwertes
## Formel: std.error = sqrt(variance/length(sample N))
## in R: std.error = sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
# lci: I --> 0.4721724; II --> 0.4868976
# uci: I --> 0.5016414; II --> 0.5171098
# mean: I --> 0.4869069; II --> 0.5020037
# std.error: I --> 0.008949626; II --> [1] 0.009175342
# Je größer N desto kleiner sind die Werte von linkem/rechtem Fehler, Standardabweichung und Mittelwert!
# Manuell korrekt nachberechnet:
# links
mean(random_data_sample_bigN) - (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
#
# rechts
mean(random_data_sample_bigN) + (stats::qt(0.95, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
#
# Mittelwert
mean(random_data_sample_bigN)
#
# Angepasst an unsymmetrische Fehlerverteilung, d.h. links 1% und rechts 4%:
# links
mean(random_data_sample_bigN) - (stats::qt(0.99, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
#
# rechts
mean(random_data_sample_bigN) + (stats::qt(0.96, df = length(random_data_sample_smallN)-1) * sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE) / length(stats::na.omit(random_data_sample_smallN))))
#
qnorm(.95)
margin_of_error <- qqnorm(.95)*(sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE)/sqrt(random_data_sample_bigN))
margin_of_error <- qqnorm(.95)*(sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE))/sqrt(random_data_sample_bigN))
margin_of_error <- qqnorm(.95)*(sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE))/sqrt(random_data_sample_bigN))
margin_of_error <- qqnorm(.95)*(sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE))/sqrt(random_data_sample_bigN))
margin_of_error <- qnorm(.95)*(sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE))/sqrt(random_data_sample_bigN))
margin_of_error
lower_bound <- mean(random_data_sample_bigN) - margin_of_error
lower_bound
lower_bound
margin_of_error <- qnorm(.95)*(sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE))/sqrt(random_data_sample_bigN))
margin_of_error
lower_bound <- mean(random_data_sample_bigN) - margin_of_error
lower_bound
qnorm(.95)
margin_of_error <- 1.644854*(sqrt(stats::var(random_data_sample_smallN, na.rm = TRUE))/sqrt(random_data_sample_bigN))
margin_of_error
margin_of_error <- 1.644854*(sqrt(stats::var(random_data_sample_smallN))/sqrt(random_data_sample_bigN))
margin_of_error
margin_of_error <- 1.644854*(sqrt(var(random_data_sample_smallN))/sqrt(random_data_sample_bigN))
margin_of_error
margin_of_error <- qnorm(0.95*(sqrt(var(random_data_sample_smallN))/sqrt(random_data_sample_bigN))
lower_bound <- mean(random_data_sample_bigN) - margin_of_error
margin_of_error <- qnorm(0.95)*(sqrt(var(random_data_sample_smallN))/sqrt(random_data_sample_bigN))
margin_of_error
sqrt(var(random_data_sample_smallN))
margin_of_error <- qnorm(0.95)*(sqrt(var(random_data_sample_smallN))/sqrt(length(random_data_sample_bigN)=)
lower_bound <- mean(random_data_sample_bigN) - margin_of_error
margin_of_error <- qnorm(0.95)*(sqrt(var(random_data_sample_smallN))/sqrt(length(random_data_sample_bigN))
margin_of_error <- qnorm(0.95)*(sqrt(var(random_data_sample_smallN))/sqrt(length(random_data_sample_bigN)))
margin_of_error
lower_bound <- mean(random_data_sample_bigN) - margin_of_error
lower_bound
alpha=0.0
z(alpha/2)
alpha=0.05
alpha/2
alpha=0.1
alpha/2
margin_of_error <- qnorm(1-alpha/2)*(sqrt(var(random_data_sample_smallN))/sqrt(length(random_data_sample_bigN)))
margin_of_error
lower_bound <- mean(random_data_sample_bigN) - margin_of_error
lower_bound
# Confidence interval 95%
alpha = 0.05
alpha/2
# Confidence interval 90%
alpha_90 = 0.1
alpha_90/2
margin_of_error_90 <- qnorm(1-alpha_90/2)*(sqrt(var(random_data_sample_smallN))/sqrt(length(random_data_sample_bigN)))
margin_of_error_90
lower_bound_90 <- mean(random_data_sample_bigN) - margin_of_error
lower_bound_90
# Confidence interval 95%
alpha_95 = 0.05
alpha_95/2
margin_of_error_95 <- qnorm(1-alpha_95/2)*(sqrt(var(random_data_sample_smallN))/sqrt(length(random_data_sample_bigN)))
margin_of_error_95
lower_bound_95 <- mean(random_data_sample_bigN) - margin_of_error
lower_bound_95
margin_of_error_90 <- qnorm(1-alpha_90/2)*(sd(random_data_sample_smallN)/sqrt(length(random_data_sample_bigN)))
margin_of_error_90
margin_of_error_90 <- qnorm(1-alpha_90/2)*(sqrt(var(random_data_sample_smallN))/sqrt(length(random_data_sample_bigN)))
margin_of_error_90
margin_of_error_95 <- qnorm(1-alpha_95/2)*(sd(random_data_sample_smallN)/sqrt(length(random_data_sample_bigN)))
margin_of_error_95
upper_bound_90 <- mean(random_data_sample_bigN) + margin_of_error
upper_bound_90
# Confidence interval 90%
alpha_90 = 0.1
alpha_90/2
margin_of_error_90 <- qnorm(1-alpha_90/2)*(sd(random_data_sample_smallN)/sqrt(length(random_data_sample_bigN)))
margin_of_error_90
lower_bound_90 <- mean(random_data_sample_bigN) - margin_of_error_90
lower_bound_90
upper_bound_90 <- mean(random_data_sample_bigN) + margin_of_error_90
upper_bound_90
# Confidence interval 95%
alpha_95 = 0.05
alpha_95/2
margin_of_error_95 <- qnorm(1-alpha_95/2)*(sd(random_data_sample_smallN)/sqrt(length(random_data_sample_bigN)))
margin_of_error_95
lower_bound_95 <- mean(random_data_sample_bigN) - margin_of_error_95
lower_bound_95
upper_bound_95 <- mean(random_data_sample_bigN) + margin_of_error_95
upper_bound_95
textENpng <- ocr("./samples/sampleEN2.png", engine = eng)
library(tesseract)
# English (default): PNG to Text
# tesseract engine Defaults to English ('eng')!
eng <- tesseract("eng")
textENpng <- ocr("./samples/sampleEN2.png", engine = eng)
setwd("C:/Code/CollectoR/CollectoR")
# English (default): PNG to Text
# tesseract engine Defaults to English ('eng')!
eng <- tesseract("eng")
textENpng <- ocr("./samples/sampleEN2.png", engine = eng)
textENpng <- ocr("./samples/sampleEN2.png", engine = eng)
setwd("C:/Code/CollectoR/CollectoR/OCR_tesseract")
# English (default): PNG to Text
# tesseract engine Defaults to English ('eng')!
eng <- tesseract("eng")
textENpng <- ocr("./samples/sampleEN2.png", engine = eng)
cat(textENpng)
#install.packages("tesseract")
library(tesseract)
setwd("C:/Code/OpenDataMInfo_TeamMarhack/Code")
textENpng <- ocr("./Data/TimoBollMaLong2017WC/image0.png", engine = eng)
setwd("C:/Code/OpenDataMInfo_TeamMarhack/")
# English (default): PNG to Text
# tesseract engine Defaults to English ('eng')!
eng <- tesseract("eng")
textENpng <- ocr("./Data/TimoBollMaLong2017WC/image0.png", engine = eng)
cat(textENpng)
textENpng <- ocr("./Data/TimoBollMaLong2017WC/image1.png", engine = eng)
cat(textENpng)
textENpng <- ocr("./Data/TimoBollMaLong2017WC/image2.png", engine = eng)
cat(textENpng)
textENpng <- ocr("./Data/TimoBollMaLong2017WC/image3.png", engine = eng)
cat(textENpng)
textENpng <- ocr("./Data/TimoBollMaLong2017WC/image4.png", engine = eng)
cat(textENpng)
